大型分布式网站架构设计与实践(陈康贤)读书笔记(2019-8-10)

知识积累,输入和输出极为重要!

单一应用架构:
	没啥好说的,所有功能都放在同一个进程(应用)里;

垂直应用架构 :
	由单一应用架构应变而来,主要是解决了流量的增长,单一机器无法满足性能;
	将原单一的整体进行细度的划分,划分成为各个模块(服务),各模块之间互不干扰;

分布式应用结构:
	由垂直应用架构演变而来,
	业务量的增长,各个服务之间避免不了互相调用,否则不同服务之间存在严重的重叠业务,代码量重复,重复造轮子,
	将业务相同的部分抽取出来独立成为一个服务,以免供彼此之间的互相调用,这样就演变成了分布式应用架构

java中序列化对ObjectOutputStream序列化时,在源代码中需要验证是否序列化如下:
    1.序列化对象是否字符串类型/数组/枚举 -> 如果是走各自的序列化; (底层这些东西都实现了序列化接口)
    2.序列化对象是都实现Serializable接口 ,否则报错 -> NotSerializableException
    3.详情查看ObjectOutputStream 1171行

HTTP : Hypertext Transfer Protocol (超文本传输协议)缩写   
    
网络接口层 -> IP(网络层) -> TCP(传输层) -> HTTP(应用层)

HTTP请求与响应步骤: http://www.google.com:80/index.html
    1.浏览器根据使用的HTTP协议,解析出url对应的域名;
    2.DNS通过域名解析,得到该域名对应的IP地址;
    3.通过url解析出对应端口号(80默认省略)
    4.浏览器发起并建立127.0.0.1:80端口的连接(三次握手);
    5.浏览器向服务器发送GET请求
    6.服务器响应浏览器请求,浏览器读取响应,并渲染网页;
    7.浏览器关闭与服务器的连接;

负载均衡常见算法:
    1.轮询法(牺牲性能)
    2.随机法(若使用轮询法,建议使用随机法替换)
    3.源地址哈希法 通过计算IP的哈希值对服务数量进行取模得到服务地址.使用此算法,在服务不变的情况下,每次的请求都会映射到同一个服务;
        根据此特性,可以在服务消费者和服务提供者之间建立有状态的session会话;
    4.加权轮询法 一般用于服务存在高配和低配配置的情况下
    5.加权随机法
    6.最小连接数法
    
  HTTP未加密的明文
  网关的作用: 接收各种HTTP请求,完成对应的权限与安全校验;
  
  分布式系统基础设施:
    分布式协作及配置管理系统Zookeeper
    分布式缓存系统
       高并发环境下,减轻数据库的压力,提高系统的响应速度和并发吞吐量;
    持久化存储
    分布式消息系统
    搜索引擎
    CDN系统
    负载均衡系统
    日志收集系统
    监控系统
    数据仓库等等
    
  分布式缓存: 
    memcache key-value 时间复杂度O(1)
    基于TCP协议之上的memcache协议通信,协议支持两种数据的传递:
        1.文本行  主要用来承载客户端的的命令以及服务端的响应
        2.非结构化数据 使用字节流的形式在客户端和服务端之间进行传输和存储
    LRU算法 : Least Recently Used : 将最近不常访问的数据剔除 ; mybatis的二级缓存默认也是使用LRU(链表+hash);
  分布式session:
    1.持久化到DB 可以保证宕机是会话不易丢失,然后系统的整体吞吐量影响大
    2.session统一存储在缓存集群 可以保证较高的读,写性能;session的时效性和缓存的失效机制很好利用; 
    
  memcached-session-manager : 
    开源高可用的Tomcat session共享解决方案(其实就是替换tomcat默认的session管理器,tomcat默认的session管理器的实现类是StandardManager);
    替换tomcat session管理器只需要在context.xml中增加一个节点配置;
        <Context>
            <Manager pathname="实现类的全限定类名" />
        </Context>
    两种模式:
        Sticky模式 : 
            每次请求都会被映射到同一台后端web server,直到该服务器宕机,这样就可以将session先存放web server本地,等到请求完成后在同步到memcache服务器;
            当web server宕机时,请求被映射到其他web server,这时候,其他web server可以从memcache server中恢复session;
        Non-Sticky模式 :
            每次请求映射的web server是不确定的,每次请求都从memcache获取session,请求处理完毕时,重新将session放入memcache;
            
  mysql数据库问题: 随着并发量的剧增,单库无法满足,并且随着规模的扩大,数据库的架构会一步一步调整 ,如下,层次逐渐递深
    访问问题: master-slave , 提高读访问响应速度
    master单点故障问题: master-master架构 
    分表:减少单表的记录条数,以便减少数据查询所需时间,提高数据库的吞吐,这就是分表
        分表策略:使用用户ID作为分表字段 , 将用户id对分表的数量进行取模(userId%分表的数量,一般分表的数量为2的次方),得出用户所在数据在哪一张分表中;
        分表解决的痛点:
            解决单个表数据量过大导致查询效率下降的问题;
            却无法解决写入问题,master承载不了写入操作压力时,即使扩展slave也无济于事;所有,新的架构出来了->分库;
    分库:分库采取的策略与分表相似,都是采用一个关键字段进行取模得到数据库的路由;
    分库分表:为了提高数据库访问的压力,又解决海量数据的存储问题
        路由策略:
            1.中间变量=user_id%(库数量*每个库的表数量) ; (此处每个库的表述数量我理解为是同一张表的分表数量)
            2.库=取整(中间变量/每个库的表数量);
            3.表=中间变量%每个库的表数量;
        新的问题:
            分布式事务
            扩容数据迁移问题
         Hbase海量数据的存出,表的分区,并发写入能力
            
  HBase : 高可靠性,高可扩展性,实时读写的**列式存储数据库**   
    HMaster
    HRegionServer
    当表的记录数增加不断变大,将会分裂成一个个region,每个region可以由startKey和endKey表示(管理数据的范围)
    HBase基本命令:
        若命令不会使用直接使用help
        进入shell命令行 : 
            [root@server-1 bin]# ./hbase shell
        查看集群状态 : 
            hbase(main):001:0> status
        创建表:
            创建一个表,并指定表的列族名称 , 格式 create '表明','列族名称1','列族名2','列族名N'
            hbase(main):003:0> create 'user','phone','info'
        列出已存在的表:
            hbase(main):009:0> list
        查看单个表的详细描述:
            hbase(main):010:0> describe 'user'
        新增列族:
            hbase(main):012:0> alter 'user',NAME=>'address' ; NAME大写
        删除列族:
            hbase(main):014:0> alter 'user',NAME=>'address',METHOD=>'delete'
        删除表:
            删除表之前必须将表禁用
            hbase(main):015:0> disable 'user'
            hbase(main):016:0> drop 'user'
        新增表数据:
            格式: put '表名','rowKey','列族:列名','值';
            hbase(main):027:0> put 'user','1','info:name','zws'
        查看数据:
            1.根据rowKey查看 
                get '表名','rowKey';
                hbase(main):028:0> get 'user','1'
            2.根据rowKey查看具体列
                get '表名','rowKey','列族,列名';
                hbase(main):029:0> get 'user','1','info:name'
            3.全表扫描
                hbase(main):031:0> scan 'user'
            4.根据表明和列族名扫描
                hbase(main):033:0> scan 'user',{COLUMNS=>'info'}
            5.查询分页
                hbase(main):036:0> scan 'user',{COLUMNS=>'info',LIMIT=>2}
        查看表记录数:
            hbase(main):030:0> count 'user'
        删除数据:
            1.删除一列数据
                格式: delete '表名','rowKey','列族:需要删除的列名'
                hbase(main):111:0> delete 'user','1','info:name'
            2.删除一行数据 , 将这表rowKey的数据全部删掉
                hbase(main):124:0> deleteall 'user','1'

  Redis:
    高性能key-value数据库 丰富的数据结构(strings,list,set,hash,sorted set等等)
    
  消息队列:
    消息队列可以作为应用通讯的一种方式; 异步 降低系统集成的耦合度,提升了分布式系统协作的效率 , 更快响应用户,提高吞吐量
    当系统处于峰值压力时,消息队列可作为缓冲,缓解集群的压力
    ActiveMQ :
        apache提供的开源消息系统
        完全JAVA实现
        很好支持J2EE的JMS规范(JMS规范相当于java中sql的规范,提供了一系列接口规范,却没有提供实现,实现都是交给各大厂商实现)
         JMS支持两种消息发送和接收模型:
            1.P2P (point-to-point) 点对点 : 基于queue的,消息生产者发送消息到消息队列,消息消费者从队列接收消息(生产者发送一条消息到queue，只有其中一个消费者能收到。)
            2.Pub/Sub (发布/订阅) : 定义如何向一个内容节点发布和订阅消息 , 内容节点成为主题(topic)    
                消息发布者将消息发布在指定的主题,消息订阅者指定订阅某个主题进行消息消费;
                与点对点不同的是,发布/订阅模式支持一对多消费;比如在kafka中多个group.id对主题进行订阅消费.不同的group.id都有自己的偏移量
     linux下安装单机版: 下载 wget http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip           
                 解压即可 : tar -xf apache-activemq-5.9.0-bin.zip 
                 启动: 找到bin目录 , 执行 activemq start | restart | status | console
     demo: com.ws.framework.activivemq.ActiveMqDemo
     集群:
        MASTER-SLAVE架构,当MASTER宕机时,master所占用的排它锁会立即释放,其中一个slave会获取该排他锁,成为新的master,对外服务;
        扩展: 硬件扩展->垂直扩展(增加集群)->broker拆分(将queue或者topic拆分为多个broker)
     
  垂直化搜索引擎: 针对企业内部的自有数据检索 
     采用NOSQL数据库,无法进行多表关联或者复杂查询问题
     
安全:
    常见web攻击手段:
        1.xss攻击(跨站脚本攻击 Cross Site Scripting) 就是在html嵌入脚本<script>
          解决方法: 对用户输入数据进行HTML转义处理
        2.CSRF(cross site request forgery) 恶意站点B使用用户已登录的站点A的cookie向站点A发送请求,使得站点A误以为该请求是用户的请求,用此方法已达到目的,比如发邮件/消息/转账等操作;
          解决方法: 
            2.1 将cookie设置为HttpOnly , JavaScript脚本/Applet等就无法读取到cookie信息,避免了攻击者伪造cookie的情况出现;
                servlet这样设置:  response.setHeader("Set-Cookie","cookie=23432432432544545;HttpOnly);
            2.2 添加token,每次访问都携带服务器生成的token
            2.3 Referer识别 
                此属性值是请求网站的地址,后端可以根据此值决定是否允许访问; 例如我在A(www.ws.com)网站点开链接B(www.baidu.com),此时请求中的referer属性值是www.ws.com , 这样百度就可以根据此值来决定是否给与访问;
                注意:只有点击链接的时候才request的头部才会有此属性;如果直接输入网址是不会产生该头属性的
                String referer = request.getHeaders("Referer);
        3.SQL注入
           预防: 1.使用预编译语句 PreparedStatement;
                2.ORM框架自动处理
                3.处理好异常,避免异常输出到页面
        4.文件上传漏洞
            用户上传非法脚本可执行文件
            防范: 对用户上传文件进行验证,如验证文件的格式(单单验证格式是不安全的),或者验证文件的魔数;       
        5.DDos攻击 分布式拒绝服务攻击,是目前最为强大,最为难以防御的攻击;
            攻击者控制大量的"肉鸡"(用户的计算机),同一时刻对某个主机进行访问,从而达到服务器瘫痪的目的;
            5.1 SYN Flood
                TCP协议三次握手
                    连接建立流程(简略): 1.客户端发送SYN到服务器 2.服务器返回SYN-ACK给客户端 3.客户端发送ACK给服务器
                    详细:
                        1.客户端发送一个包含SYN标志的TCP报文,SYN即同步(Synchronize)的意思,SYN报文会指明客户端的端口号及TCP连接的初始序列号
                        2.服务器在接收到客户端的SYN报文后,会返回一个SYN+ACK的报文,表示客户端请求被接收,同时,TCP序列号被加一,ACK及确认的意思(Acknowledgment)
                        3.客户端在接收到服务端的SYN-ACK报文后,也会返回一个ACK给服务器,同样,TCP的序列号被加一,TCP连接便建立好了,接下来就可以进行数据通信了;
                        第三步中如果服务器没有收到客户端的ACK报文,一般会进行重试,在此发送SYN-ACK报文给客户端,并会处于SYN_RECV状态,将客户端加入等待列表;一般重试3-5次,大约间隔30秒一次
                        服务器在发出SYN-ACK报文的时候,会预先分配一部分资源给即将建立的TCP连接,这个资源在等待重试期间一直保留;
                        服务器等待列表超过上限就不在接收新的SYN报文了,也就是拒绝建立新的TCP连接;
                 SYN Flood攻击就是利用TCP三次握手的过程来达成攻击的目的,伪造大量的IP地址给服务器发送SYN报文,最终导致服务器不再接受新的TCP连接;
            5.2 DNS Query Flood
                向被攻击的服务器发起海量的域名解析请求(这些域名都是随机生成的,大部分都是不存在的,伪造IP和端口);
                当解析请求超过一定的量时,会造成DNS服务器解析域名超时,这样就达到攻击目的;
            5.3 CC(Challenge Collapsar) 攻击
                基于应用层HTTP协议发起的DDos攻击,也被称为HTTP Flood;
                控制大量的肉鸡或大量的匿名HTTP代理,向服务器发起正常用户的请求直到服务器拒绝服务为止;这些Http都会有意的避开服务器的一些缓存,导致大量的查询落到DB,
                从而拖垮后端的业务处理速度;
    常用的安全算法:            
        1.数字摘要(也称为消息摘要)
            1.1 MD5
            1.2 SHA
            1.3 RSA
    
Linux命令:
    日志文件不大,可以使用cat命令 , 例如 cat 文件名  , 显示行号: cat -n 文件名; cat命令不能修改内容
    more 文件 , 可以分页的查看 按空格下一个 , enter键盘下一行; 也不能修改文件内容
    less 文件 , 支持查找内容,并且能高亮显示 , 输入 "/关键字"  查找
    tail 文件 , 显示文件末尾的数据 , -n数字 可以指定末尾多少行, 例如 tail -n10 文件 , 指定 -f参数可以不退出程序
    head 文件 , 显示文件的头部数据 , -n数组 可以指定头部多少行数据 , 例如head -n10 文件
    sort 文件 , 查看排好序的文件 , 默认是字符顺序 
        指定参数-n 则使用数字排序;
        指定参数-r 表示逆序排序; 
        指定参数-k 数字 表示对第几列进行排序 , 如sort -k 2 文件
        指定参数-t 分隔符号 , 列分隔符号 , 如 sort -k 2 -t ' ' 文件 , 表示使用空格进行分割,然后按照第二列进行排序;
    wc : 可以用来统计字符数,行数,字数  , 更多详情使用 wc --help
        指定参数-l : 统计文件的行数;
        指定参数-c : 统计文件的字节数;
        指定参数-L : 查看文件最长行的长度;
        指定参数-w : 查看文件字数;
    uniq : 查看重复出现的行,(仅针对重复出现的两行)
    grep : 字符串查找 grep 关键字 文件 (grep ws ws.log)
        指定参数-c : 显示查找的行数
    find : 查找文件 , 例如 find /usr/app -name ws.log 查找在/usr/app路径下名为ws.log的文件
           find /usr/app -name "*.log" 查找匹配的后缀文件
           find . -print 递归打印当前目录的所有文件
    whereis : whereis ws.log;
    tar :
        tar -cf ws.tar temp temp1 : 将当前目录下temp,temp1目录打包成ws.tar;
         -c :表示生成新包; 
         -f:指定包的名称;
         -x :对打好的包进行解压; 
    curl : 访问网页等,支持多种协议,HTTP,HTTPS,FTP,telnet等等 ;
        curl www.baidu.com
        指定参数 -i : 返回带header的文档
        指定参数 -I : 只返回header信息
   日志分析工具 : sed , awk
    sed : sed并不会改变原数据,除非使用重定向存储输出(主要操作行)
        sed [options] 'command' file(s) : command 为具体的文本编辑命令; file为输入的文件
        sed 's/1/66/' ws.log : 将文本中的1替换成66并输出到控制台(不会改变原数据);
        sed -n '2,6p' ws.log : 指定输出2-6行的数据 ; -n : 表示只输出指定的行;
        sed '/4/d' ws.log    : 将包含4的行删除 ;    d :表示删除
        sed '=' ws.log       : 显示文件的行号
        sed -e '/1/c\666' ws.log : 将含有1的所有行替换成666
        sed -n '/4/p' ws.log : 显示匹配4的行
        sed -i 's/4/666/' ws.log : 将原文件4代替666(危险操作,谨慎执行)
    awk : awk [option] 'pattern {action}' file => patter : 匹配规则; action : 执行的具体操作; file : 输入的文件
        awk默认使用空格将一行分割成多个列(awk主要操作列)
        -F 指定列的分隔符
        awk '{print $1}' access.log | head -10 : 打印文件第一列数据 , 显示前10条 
        awk '/4/{print $1}' ws.log | head -2 : 匹配含有4的行 并打印第一列 ,显示前2条;
        awk 'length($0)>3{print $1}' ws.log  | head -4 : 匹配行的长度大于3的行,打印第一列,显示前4条数据
    top:  查看系统的load  
        top -p 进程号 : 指定查看进程
    uptime : 查看系统的load  
        [root@server-1 ~]# uptime
         12:55:59 up 10 days, 21:31,  1 user,  load average: 0.05, 0.03, 0.05
         load average 三个值分别代表过去1分钟,5分钟,15分钟内系统的load值;
    df : 查看磁盘的剩余空间
        df -h : 按单位格式化输出
        du -d 1 -h 目录位置 : 表示递归深度(如列出指定目录的下一级) , 查看目录的占用的大小
    iostat : 查看磁盘IO
        iostat -d -k :  -k: 以KB为单位显示 ;  -d: 查看磁盘使用情况
    free : 查看内存
        free -m : 以mb为单位显示
        
  qps : 每秒查询数
  rt : 请求响应时间(response time)
  
  io.netty.eventLoop.maxPendingTasks -- 指定NioEventLoop的taskQueue的队列大小,最大值为Integer.MAX_VALUE;, 超过则使用拒绝策略
  btrace : 动态跟踪工具,JAVA应用在线故障排查
  JVM启动参数
    -verbose:gc -Xloggc:/gc.log -XX:PrintGcDetails -XX:PrintGCDateStamps
    -verbose:gc : 输出GC相关信息;
    -Xloggc:/gc.log : 指定GC的日志
    -XX:PrintGcDetails :输出GC详情
    -XX:PrintGCDateStamps : 输出GC的时间戳
  
    

        
    
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
           
               
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  