Linux 网络1/O模型简介:
    Linux的内核将所有外部设备都看做一个文件来操作，对一个文件的读写操作会调用内核提供的系统命令，返回一个file descriptor 
(fd,文件描述符)。而对一个socket 的读写也会有相应的描述符，称为socketfd ( socket描述符)，描述符就是一个数字，它指向内核中
的一个结构体(文件路径，数据区等一些属性)。
    根据UNIX网络编程对I/O 模型的分类，UNIX提供了5种1/0模型，分别如下。
    (1)阻塞I/O模型:最常用的I/O模型就是阻塞I/O模型，缺省情形下，所有文件操作都是阻塞的。我们以套接字接口为例来讲解此模型:
在进程空间中调用recvfrom, 其系 统调用直到数据包到达且被复制到应用进程的缓冲区中或者发生错误时才返回，在此期间 一直会等待，
进程在从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此被称为阻塞I/O模型，如图1-1所示。
    (2)非阻塞I/O模型: recvfrom从应用层到内核的时候，如果该缓冲区没有数据的话，就直接返回-个EWOULDBLOCK错误，一般都对非阻
塞I/O模型进行轮询检查这个状态，看内核是不是有数据到来，如图1-2所示。
    (3)I/O复用模型:Linux提供select/poll,进程通过将-~个或多个fd传递给select或poll系统调用，阻塞在selct操作上，这样select/poll
可以帮我们侦测多个fd是否处于就绪状态。select/poll 是顺序扫描fd是否就绪，而且支持的fd数量有限，因此它的使用受到了一些制约。
Linux 还提供了一个epoll系统调用，epoll 使用基于事件驱动方式代替顺序扫描，因此性能更高。当有fd就绪时，立即回调函数rollback,
如图1-3所示。
    (4)信号驱动I/O模型:首先开启套接口信号驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数(此系统调用立即返回，进
继续工作，它是非阻塞的)。当数据准备就绪时，就为该进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，
通知主循环函数处理数据，如图1-4所示。    
    (5)异步I/O: 告知内核启动某个操作，并让内核在整个操作完成后(包括将数据从内核复制到用户自己的缓冲区)通知我们。这种模型
信号驱动模型的主要区别是:信号驱动I/O由内核通知我们何时可以开始一个I/O操作;异步I/O 模型由内核通知我们I/O操作何时已经完成，
图1-5所示。
    Java NIO的核心类库多路复用器Selector是基于epoll的多路复用技术实现。

I/O多路复用:
    在I/O编程过程中，当需要同时处理多个客户端接入请求时，可以利用多线程或者I/O多路复用技术进行处理。I/O多路复用技术通过
把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程
模型比，I/O 多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了
系统的维护工作量，节省了系统资源，I/O多路复用的主要应用场景如下。
         1.服务器需要同时处理多个处于监听状态或者多个连接状态的套接字;
         2.服务器需要同时处理多种网络协议的套接字。
    epoll对select的改进:
    1.支持一个进程打开的socket描述符( FD )不受限制(仅受限于操作系统的最大文件句柄数)。
    select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_ SETSIZE设置，默认值是1024。 epoll并没有这个限制，它所
支持的FD上限是操作系统的最大文件句柄数，这个数字远远大于1024。例如，在1GB内存的机器上大约是10万个句柄左右，具体的值可以
通过cat /proc/sys/fs/file-max察看，通常情况下这个值跟系统的内存关系比较大。
    2. I/O 效率不会随着FD数目的增加而线性下降。
        传统select/poll的另一个致命弱点，就是当你拥有一个很大的socket集合时，由于网络延时或者链路空闲，任一时刻只有少部
分的socket是“活跃”的，但是select/poll 每次调用都会线性扫描全部的集合，导致效率呈现线性下降。epoll不存在这个问题，它只
会对“活跃”的socket进行操作---这是因为在内核实现中,epoll是根据每个fd上面的callback函数实现的。那么，只有“活跃”的socket
才会去主动调用callback函数，其他idle状态的socket则不会。
    3.使用mmap加速内核与用户空间的消息传递。
        无论是select、poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存复制就显得非常重要;
        epoll是通过内核和用户空间mmap同一块内存来实现的。
    4.epoll 的API更加简单。
    
使用NIO编程的优点总结如下。
    (1)客户端发起的连接操作是异步的，可以通过在多路复用器注册OP CONNECT等待后续结果，不需要像之前的客户端那样被同步阻塞。
    (2)SocketChannel的读写操作都是异步的，如果没有可读写的数据它不会同步等待，直接返回，这样I/O通信线程就可以处理其他的
链路，不需要同步等待这个链路可用。
    (3)线程模型的优化:由于JDK的Selector在Linux等主流操作系统上通过epoll实现，它没有连接句柄数的限制(只受限于操作系统的最
大句柄数或者对单个进程的句柄限 制)，这意味着一个Selector 线程可以同时处理成千上万个客户端连接，而且性能不会随着客户端的
增加而线性下降。因此，它非常适合做高性能、高负载的网络服务器。

解决粘包和拆包问题:
  TCP以流的方式进行数据传输，上层的应用协议为了对消息进行区分，往往采用如下4种方式。
    (1)消息长度固定，累计读取到长度总和为定长LEN的报文后，就认为读取到了一个完整的消息:将计数器置位，重新开始读取下一个
数据报;如FixedLengthFrameDecoder;
    (2)将回车换行符作为消息结束符，例如FTP协议，这种方式在文本协议中应用比较广泛;如:DelimiterBasedFrameDecoder
    (3)将特殊的分隔符作为消息的结束标志，回车换行符就是一种特殊的结束分隔符;
    (4)通过在消息头中定义长度字段来标识消息的总长度。LengthFieldBasedFrameDecoder解码,LengthFieldPrepender编码

    
JAVA序列化的缺陷:
    1.无法跨语言;
      由于Java序列化技术是Java语言内部的私有协议，其他语言并不支持;
    2.序列化后的码流太大

    




